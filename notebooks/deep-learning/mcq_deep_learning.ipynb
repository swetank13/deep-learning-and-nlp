{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde9f1e0-6bce-4506-bb9d-b5983f14db94",
   "metadata": {},
   "source": [
    "### <div align=\"center\">MCQ Of Deep Learning</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147e869-a528-4d51-89d5-346d09cd4533",
   "metadata": {},
   "source": [
    "1. Which `activation function` is typically used for `multi-class classification` problems?\n",
    "   - The activation function typically used for multi-class classification problems is the softmax activation function. The softmax function is applied to the output layer of the neural network when classes are mutually exclusive.\n",
    "3. What is the purpose of `Batch Normalization` in a neural network?\n",
    "   - The purpose of Batch Normalization in a neural network is to improve the speed, stability, and performance of the training process by normalizing the inputs to each layer within a mini-batch.\n",
    "5. Which of these methods is used to avoid overfitting in a neural network?\n",
    "   - Regularization\n",
    "6. Which of the following is the main purpose of `activation functions` in neural networks?\n",
    "   - The main purpose of activation functions in neural networks is to introduce non-linearity into the model, allowing it to learn and represent complex patterns in the data. Without activation functions, a neural network would behave like a linear regression model, regardless of the number of layers or neurons, and would not be able to capture the underlying non-linear relationships often present in real-world data.\n",
    "7. Which of the following is NOT a hyperparameter of neural network training?\n",
    "   - In neural network training, hyperparameters are configuration variables set before the training process begins—they cannot be learned directly from the data during training. Common hyperparameters include:Learning rate, Number of epochs, Batch size, Number of hidden layers, Number of neurons per layer, Activation function, Optimizer type.\n",
    "   - Weights and biases (since they are model parameters, not hyperparameters).\n",
    "8. What role does pooling play in a CNN?\n",
    "   - It reduces the spatial dimensions of the data while retaining important features.\n",
    "   - Pooling plays a crucial role in Convolutional Neural Networks (CNNs) by performing downsampling or dimensionality reduction on feature maps produced by convolutional layers.\n",
    "9. What is the main advantage of using `early stopping` during training?\n",
    "    - The main advantage of using early stopping during training is that it helps to prevent overfitting. Early stopping monitors the model’s performance on a validation (hold-out) set during training and automatically halts training when the validation performance stops improving.\n",
    "11. What is the key innovation introduced by the Transformer architecture?\n",
    "    - The key innovation introduced by the Transformer architecture is the self-attention mechanism, which enables the model to weigh the importance of different words or tokens in an input sequence relative to each other when processing that sequence. This allows the Transformer to efficiently capture long-range dependencies and contextual relationships without relying on recurrent or convolutional structures.\n",
    "13. What does the term 'gradient descent' refer to in neural network training?\n",
    "   - A method for updating weights to minimize error\n",
    "11. How do `Transformers handle different positions of words` in the input data?\n",
    "    - Transformers handle the different positions of words in input data by using a technique called positional encoding. Since the Transformer architecture processes all input tokens in parallel and does not inherently capture the order of tokens, positional encoding is crucial to provide information about the sequence order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab23a4-e8f4-4eef-9bce-b5967673555b",
   "metadata": {},
   "source": [
    "11. What is the main purpose of `Recurrent Neural Networks` (RNNs) in sequence modeling?\n",
    "    - Handle sequential data with memory\n",
    "12. Which of the following methods helps in hyperparameter tuning to find the optimal combination of parameters?\n",
    "    - Several methods are commonly used in hyperparameter tuning to find the optimal combination of parameters for machine learning models:\n",
    "    - Grid Search: This approach exhaustively evaluates all possible combinations of a predefined set of hyperparameter values. Each combination is tested and the one yielding the best performance (often via cross-validation) is selected. Grid search is straightforward but can be computationally expensive, especially when dealing with many hyperparameters or large datasets\n",
    "14. In the context of neural networks, what is a 'perceptron'?\n",
    "    - The simplest neural network that classifies linearly separable data\n",
    "15. Which of the following optimizers is a combination of momentum and RMSProp?\n",
    "    - Adam\n",
    "16. What does the 'ReLU' activation function do?\n",
    "    - Introduces non-linearity by outputting the positive part of the input\n",
    "    - The ReLU (Rectified Linear Unit) activation function is a widely used nonlinear activation in neural networks, especially in deep learning.\n",
    "    - What this means: If the input is positive, ReLU outputs the input itself.\n",
    "If the input is negative (or zero), ReLU outputs zero.\n",
    "17. What are the two main components in a Transformer block?\n",
    "    - Attention and Feed-Forward Network\n",
    "18. In the context of `CNNs`, what does a `kernel` do?\n",
    "    - Extracts features like edges and textures from input data\n",
    "    - In Convolutional Neural Networks (CNNs), a kernel (also called a filter) is a small matrix of learnable weights that acts as a pattern detector during the convolution operation.\n",
    "19. Which optimizer helps the model escape local minima?\n",
    "    - Optimizers that help a model escape local minima in neural network training typically do so by either introducing randomness, using adaptive learning rates, or leveraging momentum: SGD, SGD with Momentum, Adaptive Optimizers (Adam, RMSprop, Adagrad).\n",
    "    - SGD (with or without momentum) and adaptive optimizers like Adam are the most widely used algorithms for helping neural networks escape local minima during training, due to their ability to introduce randomness, adapt step sizes, and leverage previous update history\n",
    "21. What is the role of the `stride` in convolutional operations?\n",
    "    - The role of the stride in convolutional operations within convolutional neural networks (CNNs) is to control how far the convolution filter (or kernel) moves or \"slides\" across the input image or feature map at each step. Specifically, the stride determines the number of pixels the filter shifts after each operation.\n",
    "22. Why is the `softmax` function typically used in the output layer of a neural network?\n",
    "    - The softmax function is typically used in the output layer of a neural network for multi-class classification problems because it transforms the network’s raw output values (logits) into a normalized probability distribution over all possible classes. This means each output value is converted into a probability (between 0 and 1), and the sum of all probabilities equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec2179-7293-48c0-8461-c4d45bf64830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
