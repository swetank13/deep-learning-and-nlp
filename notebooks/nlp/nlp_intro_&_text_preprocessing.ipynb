{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8413e83c-0699-45c8-91e3-142aae57e5af",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Introduction to NLP</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adde38c-af40-4799-9985-08cae2e1fec4",
   "metadata": {},
   "source": [
    "#### 1.2: Three Techniques of doing NLP\n",
    "- NLP Techniques\n",
    "  1. Rules & Heuristics\n",
    "  2. Machine Learning\n",
    "  3. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476a3fb-9235-4d01-9e40-9cf1f50d71ea",
   "metadata": {},
   "source": [
    "#### 1.3: NLP Tasks\n",
    "- Converting text to number is called Embedding, computer understand only number.\n",
    "- Naive Bayes Classifier work well with text problem.\n",
    "- Below are the major NLP tasks:\n",
    "  1. Text Classification\n",
    "  2. Language Translation\n",
    "  3. Text Similarity\n",
    "  4. Language Modeling\n",
    "  5. Information Extraction\n",
    "  6. Text Summarization\n",
    "  7. Information Retrieval\n",
    "  8. Topic Modeling\n",
    "  9. Chat Bots\n",
    "  10. Voice Assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd5e64-4d8c-4032-923c-2a6b551959d3",
   "metadata": {},
   "source": [
    "#### 1.4: NLP Pipeline\n",
    "- NLP application pipeline or steps are different based on the type of the task (classification, summarization, chatbot etc.)\n",
    "- Steps in a classification task (as well as many other NLP tasks) are:\n",
    "  1. Data Acquisition\n",
    "  2. Preprocessing\n",
    "  3. Feature Extraction\n",
    "  4. Parsing & Syntax Analysis\n",
    "  5. Model Building\n",
    "  6. Post processing and Evaluation (If result is not good repeat the steps)\n",
    "  7. Deployment\n",
    "  8. Monitor & Update\n",
    "- Building a good quality NLP application requires thoughtful and iterative approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26236de3-8107-48e3-ac2e-6d04c0cbad1c",
   "metadata": {},
   "source": [
    "#### 1.5: Tools Overview\n",
    "- General guideline to use tool\n",
    "  - Hugging Face, LLM APIs, Langchain – used to build state-of-the-art, modern NLP and Gen AI applications.\n",
    "  - Spacy, Gensim – used to build classical NLP applications with low latency pipelines.\n",
    "  - NLTK – mainly used in research, teaching, and experimentation.\n",
    "  - Above are loose guidelines. Many industrial NLP applications use multiple of these tools in a single application.\n",
    "- Spacy\n",
    "  - Provides most efficient NLP algorithm for a given task. Hence if you care about the end result, go with Spacy.\n",
    "  - It is object oriented.\n",
    "  - It is user friendly.\n",
    "  - Provides most efficient NLP algorithm for a given task. Hence if you care about the end result, go with Spacy.\n",
    "  - Perfect for app developer.\n",
    "  - It is a new library and has very active user community. \n",
    "- NLTK\n",
    "  - Provides access to many algorithms. If you care about specific algo and customizations go with NLTK.\n",
    "  - It is mainly a string processing library.\n",
    "  - It is less user friendly compare to spacy.\n",
    "  - Provides access to many algorithms. If you care about specific algo and customizations go with NLTK.\n",
    "  - Perfect for researchers.\n",
    "  - It is old library and not as active as spacy.\n",
    "- NLP is a huge multidisciplinary field where a variety of tools and libraries are used to build NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da584751-e9f3-4ca5-8d9b-d91253ba56b1",
   "metadata": {},
   "source": [
    "#### 1.6: MCQ\n",
    "- What is Hugging Face best known for?\n",
    "  - Hosting and providing pre-trained transformer models\n",
    "  - Hugging Face is best known for its Transformers library and Model Hub, which host and provide access to thousands of pre-trained models for NLP and other AI tasks.\n",
    "- What is the purpose of sentiment analysis?\n",
    "  - Classifying the emotional tone of a text\n",
    "- What is the benefit of LangChain?\n",
    "  - It makes building GenAI applications easier and faster\n",
    "- What is the primary goal of Named Entity Recognition\n",
    "  - Identifying proper nouns and classifying them into categories like names, locations, and organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d51483e-2785-496b-9b47-d409c8a8fafe",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Text Pre-Processing</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9c0b2a-3eba-4e56-84ac-bc90713b7d87",
   "metadata": {},
   "source": [
    "#### 2.1: Tokenization Techniques\n",
    "- Pre-Processing stage\n",
    "  - Sentense Tokenization -> Word Tokenization -> Stop word removal -> Stemming, Lemmatization\n",
    "- Tokenization is the foundational step in NLP, breaking down text into smaller, processable units.\n",
    "- Word tokenization splits text into words, making it simple but prone to out-of-vocabulary issues.\n",
    "- Character tokenization breaks text into individual characters, providing flexibility for unknown words.\n",
    "- Subword tokenization efficiently handles rare words by splitting them into frequently used sub-parts.\n",
    "- Subword tokenization is a popular approach used by models such as BERT, GPT, etc., due to the advantages it offers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d306f-bf88-437f-bd3c-716ce84d7f4b",
   "metadata": {},
   "source": [
    "#### 2.2: Tokenization in Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbdf77f-e7ab-42e3-9046-f520b00e9fc7",
   "metadata": {},
   "source": [
    "- spacy.blank(\"en\") is used to create a blank language object. Using this we can tokenize a sentence.\n",
    "- Tokens in Spacy have attributes such as like_num, is_currency, is_alpha, etc., that help identify the type of token.\n",
    "- Spacy provides support for defining custom tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5d885-0964-403e-91ae-cf9928569d68",
   "metadata": {},
   "source": [
    "Create blank language object and tokenize words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0c2364-73d1-43da-964e-425573cef849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      "as\n",
      "it\n",
      "costs\n",
      "only\n",
      "2\n",
      "$\n",
      "per\n",
      "plate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai as it costs only 2$ per plate.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5be868-7a03-48d0-8658-191ea21eacbe",
   "metadata": {},
   "source": [
    "Creating blank language object gives a tokenizer and an empty pipeline. We will look more into language pipelines in next tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0941eac-7bd2-42e7-b359-9cdf8c8ce871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strange'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using index to grab tokens\n",
    "doc[0]\n",
    "token = doc[1]\n",
    "token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c5ec8a3-0c87-481d-a598-901f3ef2298c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d996e6-4a21-48c1-a222-b2667b1e5252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d18ddca0-e677-4913-8ca2-1b7843f06cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c561c1-70cc-4320-a09c-93b582ae6e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f66f384-fe2c-4b4a-b644-76b805c19d30",
   "metadata": {},
   "source": [
    "##### Token attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7e2e51-68c7-4aad-97d0-3f0fa8ec68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tony gave two $ to Peter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0db22c17-2225-481e-9e84-ea8f90cf77c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f915cee-2463-48b8-9fc5-c76589046896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663ea705-26b0-4d91-9cbc-46603040379b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fe1dc79-216f-4bc9-b29d-ab1aaa47e2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "two"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc[2]\n",
    "token2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b833b4a3-97ce-429d-93fe-c5c8becb033d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66c1249f-a186-4afc-8d47-1a6def2389c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "gave ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "two ==> index:  2 is_alpha: True is_punct: False like_num: True is_currency: False\n",
      "$ ==> index:  3 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "to ==> index:  4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Peter ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ". ==> index:  6 is_alpha: False is_punct: True like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i, \"is_alpha:\", token.is_alpha, \n",
    "          \"is_punct:\", token.is_punct, \n",
    "          \"like_num:\", token.like_num,\n",
    "          \"is_currency:\", token.is_currency,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9720a-b147-40b0-bcda-0ecc748cc247",
   "metadata": {},
   "source": [
    "#### 2.3: Language Processing Pipeline in Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df2856-179c-4452-bcd1-1b72a5aad5c5",
   "metadata": {},
   "source": [
    "- SpaCy provides many pre-trained pipelines. ex: spacy.load(\"en_core_web_sm\") loads a small pipeline for the English language.\n",
    "- This pipeline has components such as tok2vec, tagger, parser, ner, lemmatizer, etc.\n",
    "- token.pos_ gives the part of speech component and token.lemma_ returns the lemmatized base word.\n",
    "- doc.ents is used to retrieve the Named Entity Recognition (NER) entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2940011-e3a7-4807-a214-3cacaded6a06",
   "metadata": {},
   "source": [
    "##### Blank nlp pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30d9df44-7046-45e9-a67d-0af4ec7a3279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      "america\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "I\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba96ccfa-2bce-4e3c-a514-a6ec70e0213d",
   "metadata": {},
   "source": [
    "<img height=300 width=400 src=\"../../screenshots/spacy_blank_pipeline.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42e71464-19a5-4001-a101-054ac6ebcbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7fd002-d99c-4659-ae08-261052a76826",
   "metadata": {},
   "source": [
    "nlp.pipe_names is empty array indicating no components in the pipeline. Pipeline is something that starts with a tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8456e2-ead7-4d2b-b29d-44be96987552",
   "metadata": {},
   "source": [
    "More general diagram for nlp pipeline may look something like below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15add15-2244-4893-8d09-5e56af6258eb",
   "metadata": {},
   "source": [
    "<img height=300 width=400 src=\"../../screenshots/spacy_loaded_pipeline.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d855c4-52dd-4e1f-bcd4-1654a062149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade8bf4-d997-45ec-844f-5d0a1f32708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7197073-0697-40c5-ab4d-40fe3b755e80",
   "metadata": {},
   "source": [
    "sm in en_core_web_sm means small. There are other models available as well such as medium, large etc. Check this: https://spacy.io/usage/models#quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d61f7-daef-4615-89d0-f99efc5bac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8e5e15-f0d8-49f5-bca0-82a752fd7402",
   "metadata": {},
   "source": [
    "##### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf892e6-4e83-4820-afe0-b51b01a50a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487bb922-ff06-4d90-a18f-fb4a2f687d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19ce26-8076-4f9a-ab64-408fde32d7b2",
   "metadata": {},
   "source": [
    "#### 2.4: Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5c817-a6f7-4148-a6f0-4f1a675a0cef",
   "metadata": {},
   "source": [
    "- Stemming: Use fixed rule such as remove ing, able etc. to derive a base word\n",
    "- Lemmatization: Here we need knowledge of a language (a.k.a linguistic language) to derive a base word (a.k.a Lemma).\n",
    "- Stemming reduces words to their root form by stripping suffixes, often leading to incomplete or less meaningful roots (e.g., \"running\" → \"run\").\n",
    "- Lemmatization reduces words to their base or dictionary form (lemma) by considering the word's meaning and context (e.g., \"running\" ↦ \"run\").\n",
    "- Lemmatization is more accurate than stemming but computationally heavier, making it preferable for tasks requiring linguistic precision.\n",
    "- Stemming is faster and simpler, useful for quick text normalization when perfect accuracy isn't critical.\n",
    "- Both techniques improve text preprocessing by reducing word variations, aiding in tasks like search, classification, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f0008-dfd0-4b59-b87e-feab67f1361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\n",
    "doc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f9384-fdcc-413b-857a-6489c49c1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ec6ae-7851-46c9-aeae-ce14ad9639a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "\n",
    "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"})\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd617c4d-44de-4449-bf13-a8570a37676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc6d29-9a6c-48b9-b836-4525d0a54457",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[6].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ccc62-3422-4e64-a18f-c662213ac0b3",
   "metadata": {},
   "source": [
    "#### 2.5: Part of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3777c5-add4-4ec3-84c1-ceed988d37a7",
   "metadata": {},
   "source": [
    "POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c4c95-593c-4c90-a64f-136ca2e09c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Elon flew to mars yesterday. He carried biryani masala with him\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc3e80-eba4-4ca2-b9d1-04b1e6003577",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece02d2-a30d-4a56-9c55-0d8de81950db",
   "metadata": {},
   "source": [
    "Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de803e3-f00e-42e5-ae71-265aa2b36dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_), \" | \", token.tag_, \" | \", spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4a01e-9c6b-4e93-b1e6-39bde7782ea6",
   "metadata": {},
   "source": [
    "In below sentences Spacy figures out the past vs present tense for quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c9c36-e350-48ef-b951-d23f48e2cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"He quits the job\")\n",
    "\n",
    "print(doc[1].text, \"|\", doc[1].tag_, \"|\", spacy.explain(doc[1].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0c356-fd62-4f40-a70b-1db572ab838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"He quits the job\")\n",
    "\n",
    "print(doc[1].text, \"|\", doc[1].tag_, \"|\", spacy.explain(doc[1].tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65e6be-9809-48d3-bbac-65f533c9b974",
   "metadata": {},
   "source": [
    "Removing all SPACE, PUNCT and X token from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb8c05-8228-44f6-980a-60696d335583",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_text=\"\"\"Microsoft Corp. today announced the following results for the quarter ended December 31, 2021, as compared to the corresponding period of last fiscal year:\n",
    "\n",
    "·         Revenue was $51.7 billion and increased 20%\n",
    "·         Operating income was $22.2 billion and increased 24%\n",
    "·         Net income was $18.8 billion and increased 21%\n",
    "·         Diluted earnings per share was $2.48 and increased 22%\n",
    "“Digital technology is the most malleable resource at the world’s disposal to overcome constraints and reimagine everyday work and life,” said Satya Nadella, chairman and chief executive officer of Microsoft. “As tech as a percentage of global GDP continues to increase, we are innovating and investing across diverse and growing markets, with a common underlying technology stack and an operating model that reinforces a common strategy, culture, and sense of purpose.”\n",
    "“Solid commercial execution, represented by strong bookings growth driven by long-term Azure commitments, increased Microsoft Cloud revenue to $22.1 billion, up 32% year over year” said Amy Hood, executive vice president and chief financial officer of Microsoft.\"\"\"\n",
    "\n",
    "doc = nlp(earnings_text)\n",
    "\n",
    "filtered_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"PUNCT\", \"X\"]:\n",
    "        filtered_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d913e-1565-456e-a517-29d06a1b4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c127f-e962-476b-b9d3-9a4c3d7745e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8371f6f-6c50-4759-92cf-a528cbedc9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.vocab[96].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a8224-0ae9-4cc1-a94f-14f8747651e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text, \"|\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b225c-45a7-4ed1-90de-604abe36916a",
   "metadata": {},
   "source": [
    "#### 2.6: Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678069c-75dd-45f7-9b1a-541f0dacb5c3",
   "metadata": {},
   "source": [
    "- Stop words are common words (e.g., \"the\", \"and\", \"is\") that add little value to text analysis and are often removed to improve model efficiency.\n",
    "- Removing stop words reduces dimensionality and processing time, allowing models to focus on meaningful terms.\n",
    "- Stop word removal enhances performance in tasks like text classification, sentiment analysis, and information retrieval.\n",
    "- Careful selection of stop words is essential, as removing critical words can sometimes alter the meaning of text.\n",
    "- Popular NLP libraries like NLTK, SpaCy, and Scikit-learn offer built-in stop word lists that can be customized based on task requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c67148-e602-4d98-be7d-5a6b0039a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "len(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b82d39-cf3a-4127-968b-45233aada8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"We just opened our wings, the flying part is coming soon\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8f048-a4c4-4b75-ac97-42aeb997abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    no_stop_words = [token.text for token in doc if not token.is_stop]\n",
    "    return \" \".join(no_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dacac8-54e1-4cd5-b6a9-7b134b28915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\"Musk wants time to prepare for a trial over his\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fa411-b86a-4c1e-98cd-c3baee2aac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\"The other is not other but your divine brother\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d77ac880-1d12-4903-bf40-b314547f4641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13087, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"../../data/doj_press.json\",lines=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3e95430-f78b-4469-a43a-3fca7ba407c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>date</th>\n",
       "      <th>topics</th>\n",
       "      <th>components</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Convicted Bomb Plotter Sentenced to 30 Years</td>\n",
       "      <td>PORTLAND, Oregon. – Mohamed Osman Mohamud, 23,...</td>\n",
       "      <td>2014-10-01T00:00:00-04:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[National Security Division (NSD)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-919</td>\n",
       "      <td>$1 Million in Restitution Payments Announced t...</td>\n",
       "      <td>WASHINGTON – North Carolina’s Waccamaw River...</td>\n",
       "      <td>2012-07-25T00:00:00-04:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Environment and Natural Resources Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-1002</td>\n",
       "      <td>$1 Million Settlement Reached for Natural Reso...</td>\n",
       "      <td>BOSTON– A $1-million settlement has been...</td>\n",
       "      <td>2011-08-03T00:00:00-04:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Environment and Natural Resources Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-015</td>\n",
       "      <td>10 Las Vegas Men Indicted \\r\\nfor Falsifying V...</td>\n",
       "      <td>WASHINGTON—A federal grand jury in Las Vegas...</td>\n",
       "      <td>2010-01-08T00:00:00-05:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Environment and Natural Resources Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-898</td>\n",
       "      <td>$100 Million Settlement Will Speed Cleanup Wor...</td>\n",
       "      <td>The U.S. Department of Justice, the U.S. Envir...</td>\n",
       "      <td>2018-07-09T00:00:00-04:00</td>\n",
       "      <td>[Environment]</td>\n",
       "      <td>[Environment and Natural Resources Division]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0     None       Convicted Bomb Plotter Sentenced to 30 Years   \n",
       "1  12-919   $1 Million in Restitution Payments Announced t...   \n",
       "2  11-1002  $1 Million Settlement Reached for Natural Reso...   \n",
       "3   10-015  10 Las Vegas Men Indicted \\r\\nfor Falsifying V...   \n",
       "4   18-898  $100 Million Settlement Will Speed Cleanup Wor...   \n",
       "\n",
       "                                            contents  \\\n",
       "0  PORTLAND, Oregon. – Mohamed Osman Mohamud, 23,...   \n",
       "1    WASHINGTON – North Carolina’s Waccamaw River...   \n",
       "2        BOSTON– A $1-million settlement has been...   \n",
       "3    WASHINGTON—A federal grand jury in Las Vegas...   \n",
       "4  The U.S. Department of Justice, the U.S. Envir...   \n",
       "\n",
       "                        date         topics  \\\n",
       "0  2014-10-01T00:00:00-04:00             []   \n",
       "1  2012-07-25T00:00:00-04:00             []   \n",
       "2  2011-08-03T00:00:00-04:00             []   \n",
       "3  2010-01-08T00:00:00-05:00             []   \n",
       "4  2018-07-09T00:00:00-04:00  [Environment]   \n",
       "\n",
       "                                     components  \n",
       "0            [National Security Division (NSD)]  \n",
       "1  [Environment and Natural Resources Division]  \n",
       "2  [Environment and Natural Resources Division]  \n",
       "3  [Environment and Natural Resources Division]  \n",
       "4  [Environment and Natural Resources Division]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8f70909-fc3e-4285-ab6e-a60793f127f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>date</th>\n",
       "      <th>topics</th>\n",
       "      <th>components</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-898</td>\n",
       "      <td>$100 Million Settlement Will Speed Cleanup Wor...</td>\n",
       "      <td>The U.S. Department of Justice, the U.S. Envir...</td>\n",
       "      <td>2018-07-09T00:00:00-04:00</td>\n",
       "      <td>[Environment]</td>\n",
       "      <td>[Environment and Natural Resources Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14-1412</td>\n",
       "      <td>14 Indicted in Connection with New England Com...</td>\n",
       "      <td>A 131-count criminal indictment was unsealed t...</td>\n",
       "      <td>2014-12-17T00:00:00-05:00</td>\n",
       "      <td>[Consumer Protection]</td>\n",
       "      <td>[Civil Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17-1419</td>\n",
       "      <td>2017 Southeast Regional Animal Cruelty Prosecu...</td>\n",
       "      <td>The United States Attorney’s Office for the Mi...</td>\n",
       "      <td>2017-12-14T00:00:00-05:00</td>\n",
       "      <td>[Environment]</td>\n",
       "      <td>[Environment and Natural Resources Division, U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15-1562</td>\n",
       "      <td>21st Century Oncology to Pay $19.75 Million to...</td>\n",
       "      <td>21st Century Oncology LLC, has agreed to pay $...</td>\n",
       "      <td>2015-12-18T00:00:00-05:00</td>\n",
       "      <td>[False Claims Act, Health Care Fraud]</td>\n",
       "      <td>[Civil Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17-1404</td>\n",
       "      <td>21st Century Oncology to Pay $26 Million to Se...</td>\n",
       "      <td>21st Century Oncology Inc. and certain of its ...</td>\n",
       "      <td>2017-12-12T00:00:00-05:00</td>\n",
       "      <td>[Health Care Fraud, False Claims Act]</td>\n",
       "      <td>[Civil Division, USAO - Florida, Middle]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "4    18-898  $100 Million Settlement Will Speed Cleanup Wor...   \n",
       "7   14-1412  14 Indicted in Connection with New England Com...   \n",
       "19  17-1419  2017 Southeast Regional Animal Cruelty Prosecu...   \n",
       "22  15-1562  21st Century Oncology to Pay $19.75 Million to...   \n",
       "23  17-1404  21st Century Oncology to Pay $26 Million to Se...   \n",
       "\n",
       "                                             contents  \\\n",
       "4   The U.S. Department of Justice, the U.S. Envir...   \n",
       "7   A 131-count criminal indictment was unsealed t...   \n",
       "19  The United States Attorney’s Office for the Mi...   \n",
       "22  21st Century Oncology LLC, has agreed to pay $...   \n",
       "23  21st Century Oncology Inc. and certain of its ...   \n",
       "\n",
       "                         date                                 topics  \\\n",
       "4   2018-07-09T00:00:00-04:00                          [Environment]   \n",
       "7   2014-12-17T00:00:00-05:00                  [Consumer Protection]   \n",
       "19  2017-12-14T00:00:00-05:00                          [Environment]   \n",
       "22  2015-12-18T00:00:00-05:00  [False Claims Act, Health Care Fraud]   \n",
       "23  2017-12-12T00:00:00-05:00  [Health Care Fraud, False Claims Act]   \n",
       "\n",
       "                                           components  \n",
       "4        [Environment and Natural Resources Division]  \n",
       "7                                    [Civil Division]  \n",
       "19  [Environment and Natural Resources Division, U...  \n",
       "22                                   [Civil Division]  \n",
       "23           [Civil Division, USAO - Florida, Middle]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out those rows that do not have any topics associated with the case\n",
    "df = df[df[\"topics\"].str.len() != 0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88baacaf-6d13-4c72-8775-8a04fed0a55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4688, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "404a5ee6-08d6-4695-b98c-9025a17c8faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =df.head(100)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ed4d637-be3c-4432-af69-5082a3c9abb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents_new\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcontents\u001b[38;5;241m.\u001b[39mapply(preprocess)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"contents_new\"] = df.contents.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59844578-8f0f-46d5-bb6a-87964790f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab18983-516d-4ee4-ae1e-52c331be07d6",
   "metadata": {},
   "source": [
    "#### 2.7: Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cff77f-4f6f-41b3-a803-3f4c4b750250",
   "metadata": {},
   "source": [
    "- How to build my own NER\n",
    "  1. Simple lookup: Manually add entity in database. Then look the text, tokenize and compare.\n",
    "  2. Rule Based NER: Spacy provide a class EntityRuler to specify all the rules.\n",
    "  3. Machine Learning: CRF (Conditional Random Fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a42fc-8bd1-4bec-a0ff-596c8baa5878",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d0d57-d19b-4a2b-a5e1-7089b279bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20205f-7618-48a6-8238-f031de5fe6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee81f3-aa41-473f-9d1b-a449e6e7d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List down all the entities\n",
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47207b-6123-4ca2-b47f-6695f3fe8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Michael Bloomberg founded Bloomberg in 1982\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ce06f-b8ab-4d28-b068-bc3c4825bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire Twitter Inc for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", ent.start_char, \"|\", ent.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710eaac-cf2f-4e99-b9bd-529114dd4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tesla is going to acquire Twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e9688-7f3b-4733-81b8-7ae65b32d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = doc[2:5]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39362ddf-fb86-45e2-9830-09e43b05cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723afd14-6df7-4990-956f-a7a1caf5af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(doc, 0, 1, label=\"ORG\")\n",
    "s2 = Span(doc, 5, 6, label=\"ORG\")\n",
    "\n",
    "doc.set_ents([s1, s2], default=\"unmodified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40586b6d-4d2f-48e1-9891-fb56f4fa2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c0cb9-7ed1-41e7-a307-d119e67f6471",
   "metadata": {},
   "source": [
    "#### 2.8: Regular Expressions (Regex)\n",
    "- Regular expressions (regex) are powerful tools for pattern matching and text manipulation in NLP pipelines.\n",
    "- They enable efficient searching, extraction, and replacement of text based on specific patterns (e.g., emails, phone numbers).\n",
    "- Regex helps in data cleaning, tokenization, and text preprocessing by identifying and handling structured text.\n",
    "- Mastering regex simplifies complex text parsing tasks, reducing the need for extensive manual text processing.\n",
    "- Regular expressions are widely supported in Python (re module) and other programming languages, making them essential for text-based projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955bf34d-85f8-4d30-aa82-77874da6856e",
   "metadata": {},
   "source": [
    "#### 2.9: MCQ\n",
    "- What does doc.ents return in SpaCy?\n",
    "  - Named Entity Recognitions\n",
    "- What are stop words in NLP?\n",
    "  - Common words that are often removed from the text\n",
    "- Why are stop words removed during text preprocessing?\n",
    "  - To improve model efficiency\n",
    "- What is the benefit of character-level tokenization?\n",
    "  - Provides flexibility for unknown words\n",
    "- What does spacy.blank(\"en\") do?\n",
    "  - Creates a blank language object\n",
    "- What is the main advantage of subword tokenization?\n",
    "  - It helps in handling rare words\n",
    "- Why is lemmatization preferred over stemming in NLP?\n",
    "  - More linguistic accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244e309-9197-4692-9165-f087568a6f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
