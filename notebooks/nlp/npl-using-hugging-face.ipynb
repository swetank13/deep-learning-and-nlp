{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eddfe621-9904-4ff0-af1c-00e62480135a",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">NLP using Hugging Face Pipelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fdba5a-c632-40d7-a65a-97e6eb08810c",
   "metadata": {},
   "source": [
    "#### 4.2: Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a07abd-1ad7-436a-bae9-4d427bddc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368148cd-bfbb-497e-aa54-2742cf0a98d3",
   "metadata": {},
   "source": [
    "#### Sentiment Classificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fbac0-f4ef-4ab1-b0dd-66e05c0f78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce680dc-0bde-424d-88bd-9220b1dc2e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9987161159515381}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls(\"Pushpa 2 movie is full of violence and gave me a headache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1565d9-a3d3-4166-a5e3-1a0a4f585a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9983185529708862}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls(\"12th fail is such an inspiring movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b99da-ba02-4a92-a01a-8e0da8c6e766",
   "metadata": {},
   "source": [
    "**Specify Model Explicitly**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd6c5a-56fe-43ca-b9dd-9ccb6c7f3629",
   "metadata": {},
   "source": [
    "Enable developer mode on windows: https://learn.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d3d35-d966-4564-8033-ff695ce80b55",
   "metadata": {},
   "source": [
    "pipe = pipeline(model=\"FacebookAI/roberta-large-mnli\")\n",
    "pipe(\"This restaurant is awesome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ecaacf-4295-48b5-a5db-8d039edc8159",
   "metadata": {},
   "source": [
    "#### Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334878cf-78cd-468d-9c5e-2a420c840357",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "\n",
    "translation = translator(\"How are you?\")\n",
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0805e-02e2-4d30-be16-99c5ad3492c1",
   "metadata": {},
   "source": [
    "#### ZERO Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbeead9-10bc-42e2-83bc-bf91e85beae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"I bought the product but it is faulty, I would like to return it and get my money back\",\n",
    "    candidate_labels=[\"refund\", \"new order\", \"existing order\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6966ce2-c89f-4d82-aba2-3c1d56e1219e",
   "metadata": {},
   "source": [
    "#### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62260d-70fa-4be8-96e3-8bdb43d59c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"To become happy in life, we need to focus on healthy diet and \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1a930-8595-4b38-8b87-9c36e85dbf19",
   "metadata": {},
   "source": [
    "#### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32841e3c-d4bc-4abc-acd2-49f3448d1275",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\"ner\")\n",
    "ner(\"I am Dhaval, I work for Codebasics and live in New Jersey, USA\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3b8c7-47e4-48c2-a6ea-12e47c3177ac",
   "metadata": {},
   "source": [
    "#### 4.3: BERT, DistilBERT, RoBERTa, ALBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d2578-ad2c-48c9-9d83-7031318d4e40",
   "metadata": {},
   "source": [
    "### Comparison of NLP Models\n",
    "\n",
    "| Model       | Creator              | Architecture             | Key Features                                                                 | Strengths                                                                  | Weaknesses                                                                  |\n",
    "|-------------|----------------------|---------------------------|------------------------------------------------------------------------------|-----------------------------------------------------------------------------|------------------------------------------------------------------------------|\n",
    "| BERT        | Google AI (2018)     | Transformer (12–24 layers)| Bidirectional MLM, NSP                                                      | High performance, Strong embeddings                                        | Large size, Computationally expensive                                       |\n",
    "| DistilBERT  | Hugging Face (2019)  | Distilled BERT (6 layers) | Knowledge distillation, No NSP                                              | Faster and lightweight                                                     | Slight performance drop                                                     |\n",
    "| ALBERT      | Google Research (2019)| Optimized BERT (12–24 layers)| Parameter sharing, SOP, Dynamic masking, No NSP                          | Fewer parameters, Lower memory usage                                       | Complex architecture, Longer training                                       |\n",
    "| RoBERTa     | Facebook AI (2019)   | Enhanced BERT (24 layers) | Large data, No NSP                                                          | Outperforms BERT, Robust performance                                       | Resource-intensive, Long training                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ef5f5-16c2-4d9e-b947-28558b23e66c",
   "metadata": {},
   "source": [
    "#### Quick Guide\n",
    "- Need speed & smaller size? → DistilBERT\n",
    "- Want parameter efficiency? → ALBERT\n",
    "- Need best performance? → RoBERTa\n",
    "- Want balanced baseline? → BERT\n",
    "- BERT give the balance in most of the cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6697e-5146-4a2c-a1a1-7be457e42eac",
   "metadata": {},
   "source": [
    "### 4.4: Tokenizers in Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d12b3e1b-6eb8-46cb-b8b8-66ba744b9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f98b2d-a0c1-44fb-a25a-aba3a56829b6",
   "metadata": {},
   "source": [
    "#### DistilBERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9201e0-854c-45e1-9df0-7a874f8422c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"Happiness lies within you\"\n",
    "output = tokenizer(text)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80c8cc-1a45-473d-9e0a-a16d7ddd6324",
   "metadata": {},
   "source": [
    "#### BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96082f5a-3c06-4895-9a3b-47220b7095e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2440ebe8-deda-40ed-8f26-9c3ad76da61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8404, 3658, 2306, 2017, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Happiness lies within you\"\n",
    "\n",
    "output = tokenizer(text)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f0a54b-4198-448a-bae8-da74a8121e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] happiness lies within you [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60a309fb-09c4-4058-9eed-3e781b438f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'happiness', 'lies', 'within', 'you', '[SEP]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(output['input_ids'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1e1bf-95aa-4dcd-ba6c-4d046efd95ef",
   "metadata": {},
   "source": [
    "#### Special token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc0b9990-f979-4645-9e55-1c5a85e32e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9371166e-e2c8-42f7-8227-6be9abf1d96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f71ebef4-b760-4f20-b82a-27ee1214dddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dfc5f6d-8d91-4781-9106-44822122c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Happiness lies within you\",\n",
    "    \"I love nature\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6a3280c-2a93-4bf1-bd84-87bdbb9c34b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 8404, 3658, 2306, 2017, 102], [101, 1045, 2293, 3267, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d8edc6-5363-4738-bb80-06ee05910ae5",
   "metadata": {},
   "source": [
    "#### padding and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d25000df-d9c5-4521-9cd0-1c25d8495eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8404, 3658, 2306, 2017,  102],\n",
       "        [ 101, 1045, 2293, 3267,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(texts, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bca3b37-6499-4931-8016-d68be6aa8a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8404, 3658, 2306,  102],\n",
       "        [ 101, 1045, 2293, 3267,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(texts, padding='max_length', max_length=5, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acb8e75a-db60-40d6-a382-d904e67b0468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8404, 3658, 2306, 2017,  102,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 1045, 2293, 3267,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(texts, padding='max_length', max_length=20, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55075b61-6fa1-458e-be7d-5e851722e3e8",
   "metadata": {},
   "source": [
    "#### Supplying tokens to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "693a7fa5-6656-48df-aa05-322c54ce8352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ee7a7e5a3742c7852290fdd8b8978d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0561, -3.2456],\n",
       "        [-3.6340,  3.8584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"That phone case broke after 2 days of use\", \n",
    "    \"That herbel tea has helped me so much\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7efea5e-7a37-4f64-859d-bb37b382ed8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9933e-01, 6.7395e-04],\n",
       "        [5.5700e-04, 9.9944e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "probs = F.softmax(output.logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "665aa5ef-9dff-4f99-a3c1-f25f5605d1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes = torch.argmax(probs, dim=1).tolist()\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2627659-41b1-428e-948b-53eb2679474d",
   "metadata": {},
   "source": [
    "input text ==> tokenizer ==> tokens(token ids) ==> model ==> logits ==> post processing ==> output text\n",
    "\n",
    "Previously when we used HuggingFace pipeline we were able to do all of this with just one line of code. Above code explains the inner workings of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73cb3251-c298-4d4e-b848-ffc2aff90936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997941851615906}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"sentiment-analysis\")\n",
    "pipe(\"My dog is cute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86162acf-b5a9-41ba-9513-464a8fed3991",
   "metadata": {},
   "source": [
    "### 4.5: Model Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0ded9f-0c00-49dd-83ae-654eb2dda726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfc19f-91e1-41ca-af1f-73cf20537b46",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b2cd9-b955-4938-bb60-dd56009a5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59d47d-1e90-4d35-9249-b0b8e580cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243d058-59c8-41aa-8477-3c6a27ebe519",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5\n",
    "count = 0\n",
    "for record in dataset['train']:\n",
    "    if record['label']==0:\n",
    "        print(record)\n",
    "        if count > threshold:\n",
    "            break\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e03f1-d9a1-4751-90d5-2bd0546c762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d98cb-8a5d-4638-b856-9de93d77304c",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa16d3-a2a8-42c9-8896-145e42f1196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7c1db-2fa5-4f7c-b8a5-4800e0c78ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d3866-0a1e-457c-ab6a-4867a35c9fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for PyTorch\n",
    "train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42)\n",
    "valid_dataset = tokenized_dataset[\"validation\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709940e-2813-46b2-9d9c-4b9bf3f4f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator is used for dynamic padding per batch. For example batch 1 has texts of \n",
    "# size 10, 12 and 15. batch 2 has sizes 8, 6 and 9. Due to collator batch 1 will be padded to\n",
    "# max size of 15 whereas batch 2 will be padded to a max size of 9. The other option is to apply\n",
    "# global padding that will tax max length from the entire dataset and pad all other text to that max\n",
    "# length which is not efficient\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02bc94-3e04-4d1a-99c6-97d617d4896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2b4c6-49c6-4159-b5b4-7a54e5ba1eb9",
   "metadata": {},
   "source": [
    "#### Train / Fine Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a101f8f-45ca-4d23-b029-a57eafeed8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e60ab-5d73-43af-b30f-3dbe620d271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc9adb-4206-4a5a-bf5d-6a62831e299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79177a-a638-404d-85d0-bbcc605da6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826399cf-63be-405a-8cb4-f310822437b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may get an error if mlflow or dagshub are installed. To remove that error, you can uninstall \n",
    "# mlflow and dagshub and then restart the kernel\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2ba66-584b-4858-a3bb-9977ca8af191",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122ad25-238d-43ac-b685-3adc64aa9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "results = trainer.evaluate(test_dataset)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb38798-4dc4-48e8-9f50-5bb39b345dd4",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5549ec93-9030-49a7-8890-7cc654e5bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform predictions on new sentences\n",
    "def predict(sentences):\n",
    "    inputs = tokenizer(sentences[\"sentence1\"], sentences[\"sentence2\"], return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    return predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d8352-3ed6-4bbb-abc9-082c90e0bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = {\n",
    "    \"sentence1\": [\n",
    "        \"PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So\",\n",
    "        \"The Nasdaq composite index increased 10.73 , or 0.7 percent , to 1,514.77\"\n",
    "    ],\n",
    "    \"sentence2\": [\n",
    "        \"Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So\", \n",
    "        \"The Nasdaq Composite index, full of technology stocks, was lately up around 18 points\"\n",
    "    ]\n",
    "}\n",
    "predictions = predict(sample_sentences)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69eb32d-c46a-4b31-8ca9-8b7ee542e67d",
   "metadata": {},
   "source": [
    "### MCQ on NLP using Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5c556-118a-4b62-8fc3-738fdcb27fad",
   "metadata": {},
   "source": [
    "- Which pipeline is used for entity extraction?\n",
    "  - The pipeline used for entity extraction is the Named Entity Recognition (NER) pipeline, commonly provided by NLP frameworks like spaCy and Haystack. This pipeline processes text to identify and extract entities such as names, locations, and dates.\n",
    "- What is Hugging Face known for in the field of NLP?\n",
    "  - Providing tools for developing NLP applications\n",
    "  - Hugging Face is known for democratizing Natural Language Processing (NLP) by providing open-source libraries like Transformers, which offer thousands of pre-trained models (such as BERT, GPT, RoBERTa) for NLP tasks including text classification, translation, summarization, and question answering. They also provide tools for dataset management (Datasets library), efficient tokenization (Tokenizers library), and a collaborative Model Hub to share and deploy models. Hugging Face has built a vibrant community, making advanced NLP technology accessible and easy to use for researchers and developers worldwide.\n",
    "- Which tokenization technique is used by GPT model?\n",
    "  - The GPT model uses the Byte Pair Encoding (BPE) tokenization technique, which breaks text into subword units by iteratively merging frequent character pairs to form tokens. This approach allows GPT models to efficiently handle various words and out-of-vocabulary terms.\n",
    "- What task does AutoTokenizer.from_pretrained() perform?\n",
    "  - AutoTokenizer.from_pretrained() loads the appropriate tokenizer and its vocabulary associated with a specific pretrained model, allowing you to easily preprocess text data for use with that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8da0e-2c6f-4914-bd58-48b320d4e07e",
   "metadata": {},
   "source": [
    "### MCQ on NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804b58d-d5d7-4afb-9a23-e060d9eecb81",
   "metadata": {},
   "source": [
    "- What is the primary purpose of using a tokenizer in Hugging Face models?\n",
    "  - To split the text into tokens compatible with the model\n",
    "- Which of the following correctly represents the role of BERT in NLP?\n",
    "  - Contextual word embeddings for a wide range of tasks\n",
    "- Why is it important to remove stop words in text preprocessing?\n",
    "  - Removing stop words in text preprocessing is important because it reduces the size of the text data, improves computational efficiency, and increases the relevance of analysis by focusing on words that carry meaningful information.\n",
    "- In tokenization, what challenge does subword tokenization aim to solve?\n",
    "  - Handling out-of-vocabulary words by breaking them into smaller units\n",
    "- Which metric in TF-IDF represents the importance of a word in a specific document?\n",
    "  - In TF-IDF, the metric that represents the importance of a word in a specific document is the TF-IDF score itself. It combines Term Frequency (TF) (how often a word appears in the document) and Inverse Document Frequency (IDF) (how unique the word is across all documents). A higher TF-IDF score indicates greater importance of the word in that particular document.\n",
    "- Why is fine-tuning transformer models computationally expensive?\n",
    "  - They have millions (or billions) of parameters to optimize\n",
    "- Which of the following best describes the difference between Bag of Words (BoW) and TF-IDF?\n",
    "  - BoW counts word occurrences; TF-IDF also considers word importance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
